---
alwaysApply: true
---
# üß† BACKEND + AI MASTER PROMPT

### üöÄ PROJECT GOAL

Develop a **complete, production-ready backend system** using **FastAPI** that combines:

1. AI-Powered Multilingual Translation & Localization (for 22 Indian languages only)
2. Cultural & Domain Adaptation
3. Speech-to-Text (STT) + Text-to-Speech (TTS) for accessibility
4. Feedback-based model evaluation & continuous retraining
5. Full local storage (no AWS or cloud dependencies)
6. PostgreSQL database persistence
7. Secure JWT authentication with RBAC (Admin, Uploader, Reviewer)

All models and assets must run **locally** on your **DigitalOcean Linux server** ‚Äî GPU optional but supported.

---

## ‚öôÔ∏è TECH STACK OVERVIEW

| Layer           | Technology                                                             |
| :-------------- | :--------------------------------------------------------------------- |
| Framework       | **FastAPI** (Python 3.11)                                              |
| Database        | **PostgreSQL 15+** (via SQLAlchemy + Alembic)                          |
| Storage         | **Local filesystem** (`/app/storage/uploads`, `/app/storage/outputs`)  |
| Authentication  | **OAuth2 + JWT (passlib bcrypt)**                                      |
| AI / NLP Models | **IndicBERT**, **mBART-Indic**, **IndicTrans2**, **NLLB-Indic subset** |
| Speech Models   | **Whisper (STT)**, **VITS or Tacotron2 + HiFi-GAN (TTS)**              |
| ML Frameworks   | PyTorch + HuggingFace Transformers                                     |
| Evaluation      | BLEU, COMET scores                                                     |
| MLOps           | MLflow (optional), retraining hooks                                    |
| Monitoring      | Prometheus metrics endpoint + structured logs                          |
| Logging         | Rich + Loguru                                                          |
| Version Control | Git + DVC (for datasets)                                               |


## ‚öôÔ∏è .ENV CONFIGURATION (Local Setup)

```
DATABASE_URL=postgresql://username:password@postgres:5432/localizer
SECRET_KEY=supersecretkey
REDIS_URL=redis://redis:6379/0
STORAGE_DIR=/app/storage
UPLOAD_DIR=/app/storage/uploads
OUTPUT_DIR=/app/storage/outputs
JWT_EXPIRATION=3600
ENVIRONMENT=production
```

---

## üß© SUPPORTED LANGUAGES (22+ INDIAN LANGUAGES ONLY)

The system must **strictly** support **only** the following languages (ISO 639-1 codes):

```python
SUPPORTED_LANGUAGES = {
  "as": "Assamese",
  "bn": "Bengali",
  "brx": "Bodo",
  "doi": "Dogri",
  "gu": "Gujarati",
  "hi": "Hindi",
  "kn": "Kannada",
  "ks": "Kashmiri",
  "kok": "Konkani",
  "mai": "Maithili",
  "ml": "Malayalam",
  "mni": "Manipuri",
  "mr": "Marathi",
  "ne": "Nepali",
  "or": "Odia",
  "pa": "Punjabi",
  "sa": "Sanskrit",
  "sat": "Santali",
  "sd": "Sindhi",
  "ta": "Tamil",
  "te": "Telugu",
  "ur": "Urdu"
}
```

**If a user requests translation or TTS for any non-supported language, return HTTP 400:**

```json
{"error": "Language not supported ‚Äî choose one of 22 Indian languages"}
```

Also expose an endpoint:

```python
@app.get("/supported-languages")
def supported_languages():
    return {"languages": SUPPORTED_LANGUAGES}
```

---

## üß† AI MODEL INTEGRATION DETAILS

### üß© Translation Engine (NLP)

* **Models:**

  * `ai4bharat/IndicTrans2-en-indic`
  * `ai4bharat/IndicTrans2-indic-en`
  * `ai4bharat/IndicBERT`
  * `llama3`
  * `facebook/nllb-200-distilled-600M` (restrict to Indic pairs)
* **Framework:** PyTorch + HuggingFace Transformers
* **Pipeline:**

  1. Load model + tokenizer at startup
  2. Translate sentence-wise with batching
  3. Apply contextual post-processing (NER-based term preservation)
  4. Save output as JSON file per target language

Example pseudo-code:

```python
model = AutoModelForSeq2SeqLM.from_pretrained("ai4bharat/IndicTrans2-en-indic")
tokenizer = AutoTokenizer.from_pretrained("ai4bharat/IndicTrans2-en-indic")
translated = tokenizer.batch_decode(model.generate(inputs), skip_special_tokens=True)
```

---

### üß© Context & Cultural Localization Layer

* Use JSON domain vocabulary: `/app/data/vocabs/<domain>.json`
* Map technical words (e.g., healthcare, construction)
* Modify idioms/phrases regionally:

  * Example: ‚ÄúElectrician safety gear‚Äù ‚Üí ‚Äú‡§µ‡§ø‡§¶‡•ç‡§Ø‡•Å‡§§ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ ‡§â‡§™‡§ï‡§∞‡§£‚Äù (Hindi)
* Implement a rule-based layer for Indian cultural adaptation.

---

### üó£ Speech Layer

#### Speech-to-Text (STT)

* Model: `openai/whisper-large-v3`
* Fine-tuned for Indian accents (Hindi, Tamil, Bengali, etc.)
* Functionality:

  * Accept `.wav`, `.mp3`, or `.mp4`
  * Transcribe to text (UTF-8)
  * Save as `transcript.txt` in `outputs/<job_id>/`

#### Text-to-Speech (TTS)

* Model: `coqui-ai/TTS` or `VITS/Tacotron2 + HiFi-GAN`
* Use `IndicTTS` dataset or multilingual voicebanks
* Generate `.mp3` output for target language voice
* Example command:

```python
tts_model.tts_to_file(text=translated_text, speaker="hi_male", file_path="/outputs/tts_hi.mp3")
```

---

## üìä EVALUATION & RETRAINING

### BLEU/COMET Evaluation

* Run local scoring via SacreBLEU & COMET
* Store results in PostgreSQL (`evaluations` table)

### Retraining Endpoint

* `/retrain/trigger` ‚Üí Launch Celery task to execute `scripts/retrain.sh`
* Retrain model based on feedback and evaluation score thresholds

Sample retraining trigger:

```bash
python retrain_model.py --domain healthcare --epochs 3
```

---

## üß± DATABASE SCHEMA (PostgreSQL)

| Table          | Columns                                             |
| :------------- | :-------------------------------------------------- |
| `users`        | id, username, password_hash, role                   |
| `files`        | id, filename, path, domain, uploader_id, created_at |
| `jobs`         | id, file_id, type, status, progress, result_path    |
| `translations` | id, job_id, language, output_path, model_used       |
| `feedback`     | id, job_id, user_id, rating, comments, corrections  |
| `evaluations`  | id, job_id, bleu, comet, created_at                 |

---

## üîå FULL API ENDPOINTS

| Endpoint               | Method | Description                            |
| :--------------------- | :----- | :------------------------------------- |
| `/auth/register`       | POST   | Register new user (admin only)         |
| `/auth/login`          | POST   | User login + JWT                       |
| `/content/upload`      | POST   | Upload content to local `/uploads`     |
| `/detect-language`     | POST   | Auto-detect source language            |
| `/translate`           | POST   | Translate to selected target languages |
| `/localize/context`    | POST   | Apply domain & cultural localization   |
| `/speech/stt`          | POST   | Speech ‚Üí text (Whisper)                |
| `/speech/tts`          | POST   | Text ‚Üí speech (VITS/TTS)               |
| `/feedback`            | POST   | Store user feedback                    |
| `/evaluate/run`        | POST   | Compute BLEU/COMET score               |
| `/retrain/trigger`     | POST   | Launch retraining pipeline             |
| `/jobs/{id}`           | GET    | Check Celery job status                |
| `/metrics`             | GET    | Prometheus metrics                     |
| `/supported-languages` | GET    | Return 22 supported language codes     |

---

## üìÇ LOCAL STORAGE STRUCTURE

```
/app/storage/
 ‚îú‚îÄ‚îÄ uploads/
 ‚îÇ   ‚îú‚îÄ‚îÄ <file_id>/
 ‚îÇ   ‚îÇ    ‚îú‚îÄ‚îÄ original.pdf
 ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ metadata.json
 ‚îî‚îÄ‚îÄ outputs/
      ‚îú‚îÄ‚îÄ <job_id>/
      ‚îÇ    ‚îú‚îÄ‚îÄ translation_hi.json
      ‚îÇ    ‚îú‚îÄ‚îÄ tts_hi.mp3
      ‚îÇ    ‚îú‚îÄ‚îÄ transcript.txt
      ‚îÇ    ‚îî‚îÄ‚îÄ logs.txt
```

---

## üîß TASK FLOW (Celery)

1Ô∏è‚É£ Upload file ‚Üí store metadata
2Ô∏è‚É£ Detect language
3Ô∏è‚É£ Trigger translation tasks per target language
4Ô∏è‚É£ Apply localization layer
5Ô∏è‚É£ Run STT or TTS if requested
6Ô∏è‚É£ Store output ‚Üí record in DB
7Ô∏è‚É£ Notify job complete
8Ô∏è‚É£ Collect feedback & evaluation

Each Celery task must log progress and update PostgreSQL status (`queued`, `running`, `completed`, `failed`).

---

## üß™ ERROR HANDLING

* Invalid file format ‚Üí HTTP 415
* Unsupported language ‚Üí HTTP 400
* Invalid JWT / expired ‚Üí HTTP 401
* Missing domain vocabulary ‚Üí warning log
* Failed translation ‚Üí auto retry (3 attempts)

---

## üìà MONITORING

* `/metrics` exposes:

  * Translation job duration (histogram)
  * Celery queue size
  * BLEU/COMET average
  * Active models loaded
* Logs stored in `/logs/app.log`

---

## üß∞ LOCAL RUN COMMANDS

```bash
docker-compose up --build
```

Access the backend:
‚û°Ô∏è `http://localhost:8000/docs`

For production (DigitalOcean):

```bash
sudo docker-compose up -d
sudo systemctl enable docker
```

---

## üì¶ REQUIREMENTS.TXT EXAMPLE

```
fastapi
uvicorn
sqlalchemy
alembic
psycopg2-binary
celery
redis
pydantic
passlib[bcrypt]
python-jose[cryptography]
transformers
torch
whisper
coqui-tts
sacrebleu
comet-ml
prometheus-client
loguru
rich
```

---

## ‚úÖ EXPECTED OUTPUT

After Cursor/Copilot builds the backend:

* You get a **fully working FastAPI + AI system** that:

  * Translates & localizes only 22 Indian languages
  * Performs real Whisper-based STT
  * Generates real audio using TTS
  * Stores files locally under `/storage/`
  * Supports Celery background jobs
  * Exposes `/supported-languages` endpoint
  * Is production-deployable on your DigitalOcean Linux server