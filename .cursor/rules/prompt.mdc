---
alwaysApply: true
---
# ğŸ§  BACKEND + AI MASTER PROMPT

### ğŸš€ PROJECT GOAL

Develop a **complete, production-ready backend system** using **FastAPI** that combines:

1. AI-Powered Multilingual Translation & Localization (for 22 Indian languages only)
2. Cultural & Domain Adaptation
3. Speech-to-Text (STT) + Text-to-Speech (TTS) for accessibility
4. Feedback-based model evaluation & continuous retraining
5. Full local storage (no AWS or cloud dependencies)
6. PostgreSQL database persistence
7. Celery + Redis for background task queue
8. Secure JWT authentication with RBAC (Admin, Uploader, Reviewer)

All models and assets must run **locally** on your **DigitalOcean Linux server** â€” GPU optional but supported.

---

## âš™ï¸ TECH STACK OVERVIEW

| Layer           | Technology                                                             |
| :-------------- | :--------------------------------------------------------------------- |
| Framework       | **FastAPI** (Python 3.11)                                              |
| Database        | **PostgreSQL 15+** (via SQLAlchemy + Alembic)                          |
| Task Queue      | **Celery + Redis**                                                     |
| Storage         | **Local filesystem** (`/app/storage/uploads`, `/app/storage/outputs`)  |
| Authentication  | **OAuth2 + JWT (passlib bcrypt)**                                      |
| AI / NLP Models | **IndicBERT**, **mBART-Indic**, **IndicTrans2**, **NLLB-Indic subset** |
| Speech Models   | **Whisper (STT)**, **VITS or Tacotron2 + HiFi-GAN (TTS)**              |
| ML Frameworks   | PyTorch + HuggingFace Transformers                                     |
| Evaluation      | BLEU, COMET scores                                                     |
| MLOps           | MLflow (optional), retraining hooks                                    |
| Deployment      | Docker + docker-compose                                                |
| Monitoring      | Prometheus metrics endpoint + structured logs                          |
| Logging         | Rich + Loguru                                                          |
| Version Control | Git + DVC (for datasets)                                               |

---

## ğŸ“¦ DIRECTORY STRUCTURE

```
/backend
 â”œâ”€â”€ app/
 â”‚   â”œâ”€â”€ main.py
 â”‚   â”œâ”€â”€ core/
 â”‚   â”‚   â”œâ”€â”€ config.py
 â”‚   â”‚   â”œâ”€â”€ db.py
 â”‚   â”‚   â”œâ”€â”€ security.py
 â”‚   â”‚   â””â”€â”€ celery_app.py
 â”‚   â”œâ”€â”€ routes/
 â”‚   â”‚   â”œâ”€â”€ auth.py
 â”‚   â”‚   â”œâ”€â”€ content.py
 â”‚   â”‚   â”œâ”€â”€ translation.py
 â”‚   â”‚   â”œâ”€â”€ speech.py
 â”‚   â”‚   â”œâ”€â”€ feedback.py
 â”‚   â”‚   â””â”€â”€ retraining.py
 â”‚   â”œâ”€â”€ models/
 â”‚   â”œâ”€â”€ schemas/
 â”‚   â”œâ”€â”€ services/
 â”‚   â”‚   â”œâ”€â”€ nlp_engine.py
 â”‚   â”‚   â”œâ”€â”€ speech_engine.py
 â”‚   â”‚   â”œâ”€â”€ localization.py
 â”‚   â”‚   â””â”€â”€ retrain_manager.py
 â”‚   â””â”€â”€ utils/
 â”‚       â”œâ”€â”€ file_manager.py
 â”‚       â”œâ”€â”€ logger.py
 â”‚       â””â”€â”€ metrics.py
 â”œâ”€â”€ storage/
 â”‚   â”œâ”€â”€ uploads/
 â”‚   â””â”€â”€ outputs/
 â”œâ”€â”€ scripts/
 â”‚   â””â”€â”€ retrain.sh
 â”œâ”€â”€ docker-compose.yml
 â”œâ”€â”€ Dockerfile
 â”œâ”€â”€ requirements.txt
 â””â”€â”€ README.md
```

---

## âš™ï¸ .ENV CONFIGURATION (Local Setup)

```
DATABASE_URL=postgresql://username:password@postgres:5432/localizer
SECRET_KEY=supersecretkey
REDIS_URL=redis://redis:6379/0
STORAGE_DIR=/app/storage
UPLOAD_DIR=/app/storage/uploads
OUTPUT_DIR=/app/storage/outputs
JWT_EXPIRATION=3600
ENVIRONMENT=production
```

---

## ğŸ§© SUPPORTED LANGUAGES (22+ INDIAN LANGUAGES ONLY)

The system must **strictly** support **only** the following languages (ISO 639-1 codes):

```python
SUPPORTED_LANGUAGES = {
  "as": "Assamese",
  "bn": "Bengali",
  "brx": "Bodo",
  "doi": "Dogri",
  "gu": "Gujarati",
  "hi": "Hindi",
  "kn": "Kannada",
  "ks": "Kashmiri",
  "kok": "Konkani",
  "mai": "Maithili",
  "ml": "Malayalam",
  "mni": "Manipuri",
  "mr": "Marathi",
  "ne": "Nepali",
  "or": "Odia",
  "pa": "Punjabi",
  "sa": "Sanskrit",
  "sat": "Santali",
  "sd": "Sindhi",
  "ta": "Tamil",
  "te": "Telugu",
  "ur": "Urdu"
}
```

**If a user requests translation or TTS for any non-supported language, return HTTP 400:**

```json
{"error": "Language not supported â€” choose one of 22 Indian languages"}
```

Also expose an endpoint:

```python
@app.get("/supported-languages")
def supported_languages():
    return {"languages": SUPPORTED_LANGUAGES}
```

---

## ğŸ§  AI MODEL INTEGRATION DETAILS

### ğŸ§© Translation Engine (NLP)

* **Models:**

  * `ai4bharat/IndicTrans2-en-indic`
  * `ai4bharat/IndicTrans2-indic-en`
  * `ai4bharat/IndicBERT`
  * `facebook/nllb-200-distilled-600M` (restrict to Indic pairs)
* **Framework:** PyTorch + HuggingFace Transformers
* **Pipeline:**

  1. Load model + tokenizer at startup
  2. Translate sentence-wise with batching
  3. Apply contextual post-processing (NER-based term preservation)
  4. Save output as JSON file per target language

Example pseudo-code:

```python
model = AutoModelForSeq2SeqLM.from_pretrained("ai4bharat/IndicTrans2-en-indic")
tokenizer = AutoTokenizer.from_pretrained("ai4bharat/IndicTrans2-en-indic")
translated = tokenizer.batch_decode(model.generate(inputs), skip_special_tokens=True)
```

---

### ğŸ§© Context & Cultural Localization Layer

* Use JSON domain vocabulary: `/app/data/vocabs/<domain>.json`
* Map technical words (e.g., healthcare, construction)
* Modify idioms/phrases regionally:

  * Example: â€œElectrician safety gearâ€ â†’ â€œà¤µà¤¿à¤¦à¥à¤¯à¥à¤¤ à¤¸à¥à¤°à¤•à¥à¤·à¤¾ à¤‰à¤ªà¤•à¤°à¤£â€ (Hindi)
* Implement a rule-based layer for Indian cultural adaptation.

---

### ğŸ—£ Speech Layer

#### Speech-to-Text (STT)

* Model: `openai/whisper-large-v3`
* Fine-tuned for Indian accents (Hindi, Tamil, Bengali, etc.)
* Functionality:

  * Accept `.wav`, `.mp3`, or `.mp4`
  * Transcribe to text (UTF-8)
  * Save as `transcript.txt` in `outputs/<job_id>/`

#### Text-to-Speech (TTS)

* Model: `coqui-ai/TTS` or `VITS/Tacotron2 + HiFi-GAN`
* Use `IndicTTS` dataset or multilingual voicebanks
* Generate `.mp3` output for target language voice
* Example command:

```python
tts_model.tts_to_file(text=translated_text, speaker="hi_male", file_path="/outputs/tts_hi.mp3")
```

---

## ğŸ“Š EVALUATION & RETRAINING

### BLEU/COMET Evaluation

* Run local scoring via SacreBLEU & COMET
* Store results in PostgreSQL (`evaluations` table)

### Retraining Endpoint

* `/retrain/trigger` â†’ Launch Celery task to execute `scripts/retrain.sh`
* Retrain model based on feedback and evaluation score thresholds

Sample retraining trigger:

```bash
python retrain_model.py --domain healthcare --epochs 3
```

---

## ğŸ§± DATABASE SCHEMA (PostgreSQL)

| Table          | Columns                                             |
| :------------- | :-------------------------------------------------- |
| `users`        | id, username, password_hash, role                   |
| `files`        | id, filename, path, domain, uploader_id, created_at |
| `jobs`         | id, file_id, type, status, progress, result_path    |
| `translations` | id, job_id, language, output_path, model_used       |
| `feedback`     | id, job_id, user_id, rating, comments, corrections  |
| `evaluations`  | id, job_id, bleu, comet, created_at                 |

---

## ğŸ”Œ FULL API ENDPOINTS

| Endpoint               | Method | Description                            |
| :--------------------- | :----- | :------------------------------------- |
| `/auth/register`       | POST   | Register new user (admin only)         |
| `/auth/login`          | POST   | User login + JWT                       |
| `/content/upload`      | POST   | Upload content to local `/uploads`     |
| `/detect-language`     | POST   | Auto-detect source language            |
| `/translate`           | POST   | Translate to selected target languages |
| `/localize/context`    | POST   | Apply domain & cultural localization   |
| `/speech/stt`          | POST   | Speech â†’ text (Whisper)                |
| `/speech/tts`          | POST   | Text â†’ speech (VITS/TTS)               |
| `/feedback`            | POST   | Store user feedback                    |
| `/evaluate/run`        | POST   | Compute BLEU/COMET score               |
| `/retrain/trigger`     | POST   | Launch retraining pipeline             |
| `/jobs/{id}`           | GET    | Check Celery job status                |
| `/metrics`             | GET    | Prometheus metrics                     |
| `/supported-languages` | GET    | Return 22 supported language codes     |

---

## ğŸ“‚ LOCAL STORAGE STRUCTURE

```
/app/storage/
 â”œâ”€â”€ uploads/
 â”‚   â”œâ”€â”€ <file_id>/
 â”‚   â”‚    â”œâ”€â”€ original.pdf
 â”‚   â”‚    â””â”€â”€ metadata.json
 â””â”€â”€ outputs/
      â”œâ”€â”€ <job_id>/
      â”‚    â”œâ”€â”€ translation_hi.json
      â”‚    â”œâ”€â”€ tts_hi.mp3
      â”‚    â”œâ”€â”€ transcript.txt
      â”‚    â””â”€â”€ logs.txt
```

---

## ğŸ”§ TASK FLOW (Celery)

1ï¸âƒ£ Upload file â†’ store metadata
2ï¸âƒ£ Detect language
3ï¸âƒ£ Trigger translation tasks per target language
4ï¸âƒ£ Apply localization layer
5ï¸âƒ£ Run STT or TTS if requested
6ï¸âƒ£ Store output â†’ record in DB
7ï¸âƒ£ Notify job complete
8ï¸âƒ£ Collect feedback & evaluation

Each Celery task must log progress and update PostgreSQL status (`queued`, `running`, `completed`, `failed`).

---

## ğŸ§ª ERROR HANDLING

* Invalid file format â†’ HTTP 415
* Unsupported language â†’ HTTP 400
* Invalid JWT / expired â†’ HTTP 401
* Missing domain vocabulary â†’ warning log
* Failed translation â†’ auto retry (3 attempts)

---

## ğŸ“ˆ MONITORING

* `/metrics` exposes:

  * Translation job duration (histogram)
  * Celery queue size
  * BLEU/COMET average
  * Active models loaded
* Logs stored in `/logs/app.log`

---

## ğŸ§° LOCAL RUN COMMANDS

```bash
docker-compose up --build
```

Access the backend:
â¡ï¸ `http://localhost:8000/docs`

For production (DigitalOcean):

```bash
sudo docker-compose up -d
sudo systemctl enable docker
```

---

## ğŸ“¦ REQUIREMENTS.TXT EXAMPLE

```
fastapi
uvicorn
sqlalchemy
alembic
psycopg2-binary
celery
redis
pydantic
passlib[bcrypt]
python-jose[cryptography]
transformers
torch
whisper
coqui-tts
sacrebleu
comet-ml
prometheus-client
loguru
rich
```

---

## âœ… EXPECTED OUTPUT

After Cursor/Copilot builds the backend:

* You get a **fully working FastAPI + AI system** that:

  * Translates & localizes only 22 Indian languages
  * Performs real Whisper-based STT
  * Generates real audio using TTS
  * Stores files locally under `/storage/`
  * Supports Celery background jobs
  * Exposes `/supported-languages` endpoint
  * Is production-deployable on your DigitalOcean Linux server